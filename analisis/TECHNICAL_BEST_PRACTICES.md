# üìö BEST PRACTICES & TECHNICAL INSIGHTS
## Extra√≠dos de Materiales de Referencia

---

## üéØ LLM ENGINEERING BEST PRACTICES

### Del "LLM Engineer's Handbook" (Iusztin & Labonne, 2024)

#### **1. RAG Pipeline Design Principles**

##### **Chunking Strategy**
```python
# ‚ùå BAD: Fixed-size chunking sin overlap
chunks = text.split(every=512)

# ‚úÖ GOOD: Recursive chunking con overlap y metadata
from langchain.text_splitter import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=512,
    chunk_overlap=64,  # 12.5% overlap para preservar contexto
    length_function=len,
    separators=["\n\n", "\n", ".", " ", ""],  # Prioriza separadores naturales
    keep_separator=True
)

chunks = splitter.create_documents(
    texts=[text],
    metadatas=[{"source": "doc.pdf", "page": 1, "department": "Cusco"}]
)
```

**Rationale:**
- Overlap preserva contexto en bordes de chunks
- Separadores naturales mantienen coherencia sem√°ntica
- Metadata facilita filtering y source citation

##### **Embedding Selection**
```python
# Para casos multiling√ºes (espa√±ol + ingl√©s):
model_name = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
# - 768 dimensiones
# - Trained en 50+ lenguajes
# - Balance entre calidad y velocidad
# - ~420MB de modelo

# Para solo espa√±ol (alternativa):
model_name = "hiiamsid/sentence_similarity_spanish_es"
# - Optimizado para espa√±ol
# - Menor tama√±o

from langchain.embeddings import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings(
    model_name=model_name,
    model_kwargs={'device': 'cuda'},  # o 'cpu'
    encode_kwargs={'normalize_embeddings': True}  # Facilita cosine similarity
)
```

##### **Vector Store Configuration**
```python
# FAISS para local development/demo:
from langchain.vectorstores import FAISS

vector_store = FAISS.from_documents(
    documents=chunks,
    embedding=embeddings,
    distance_strategy="COSINE"  # vs DOT_PRODUCT o EUCLIDEAN
)

# Guardar para persistencia
vector_store.save_local("./vector_db/faiss_index")

# Chroma para production (mejor persistencia):
import chromadb
from langchain.vectorstores import Chroma

chroma_client = chromadb.PersistentClient(path="./vector_db/chroma")
vector_store = Chroma(
    client=chroma_client,
    collection_name="peru_tourism",
    embedding_function=embeddings,
    collection_metadata={"hnsw:space": "cosine"}
)
```

**Trade-offs:**
| Feature | FAISS | Chroma |
|---------|-------|--------|
| Speed | ‚ö°‚ö°‚ö° Fastest | ‚ö°‚ö° Fast |
| Persistence | Manual save | Auto-persist |
| Filtering | Limited | Rich metadata |
| Production | Needs wrapper | Production-ready |
| Setup | Simple | Slightly complex |

**Recomendaci√≥n:** FAISS para prototipado, Chroma para producci√≥n.

##### **Retrieval Strategy**
```python
# ‚ùå BAD: Naive similarity search
results = vector_store.similarity_search(query, k=5)

# ‚úÖ GOOD: Multi-stage retrieval con reranking
# Stage 1: Cast wide net
candidates = vector_store.similarity_search(
    query, 
    k=20,  # Sobre-recuperar
    filter={"department": "Cusco"}  # Metadata filtering si aplica
)

# Stage 2: Rerank con cross-encoder (opcional pero poderoso)
from sentence_transformers import CrossEncoder

reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
scores = reranker.predict([(query, doc.page_content) for doc in candidates])

# Ordenar por score y tomar top-k
ranked_docs = [doc for _, doc in sorted(zip(scores, candidates), reverse=True)]
final_docs = ranked_docs[:5]
```

**Rationale:**
- Stage 1: Embeddings son fast pero aproximados
- Stage 2: Cross-encoders son lentos pero precisos
- Best of both worlds: Speed + Accuracy

##### **Hybrid Search (Semantic + Keyword)**
```python
from langchain.retrievers import BM25Retriever, EnsembleRetriever

# BM25 para keyword matching (importante para nombres propios)
bm25_retriever = BM25Retriever.from_documents(chunks)
bm25_retriever.k = 10

# Semantic retriever
semantic_retriever = vector_store.as_retriever(search_kwargs={"k": 10})

# Ensemble combina ambos
hybrid_retriever = EnsembleRetriever(
    retrievers=[bm25_retriever, semantic_retriever],
    weights=[0.3, 0.7]  # M√°s peso a semantic, pero keyword ayuda
)

results = hybrid_retriever.get_relevant_documents(query)
```

**Casos de uso:**
- Query: "¬øQu√© ver en Machu Picchu?" ‚Üí Semantic domina
- Query: "Restaurante Astrid y Gast√≥n en Lima" ‚Üí Keyword critical

#### **2. Prompt Engineering for RAG**

##### **System Prompt Template**
```python
SYSTEM_PROMPT = """Eres PeruGuide AI, un asistente tur√≠stico experto en Per√∫.

TU ROL:
- Proporcionar informaci√≥n precisa y verificable sobre turismo en Per√∫
- Basar TODAS tus respuestas en el contexto proporcionado
- Citar la fuente espec√≠fica de cada informaci√≥n
- Ser honesto cuando no tienes informaci√≥n suficiente

DIRECTRICES:
1. FACTUALIDAD: Solo usa informaci√≥n del contexto. Si el contexto no contiene 
   la informaci√≥n necesaria, di: "No tengo informaci√≥n suficiente para responder 
   con certeza a esa pregunta."

2. CITACI√ìN: Despu√©s de cada afirmaci√≥n importante, incluye la fuente entre 
   corchetes [Fuente: nombre_documento, p√°gina X]

3. ESTRUCTURA: 
   - Respuestas claras y concisas
   - Usa bullets para listas
   - Divide informaci√≥n compleja en secciones

4. TONO: Amigable pero profesional, como un gu√≠a tur√≠stico experimentado

5. IDIOMA: Responde en el mismo idioma de la pregunta

CONTEXTO PROPORCIONADO:
{context}

PREGUNTA DEL USUARIO:
{question}

RESPUESTA:"""
```

##### **Few-Shot Examples (para casos complejos)**
```python
FEW_SHOT_EXAMPLES = """
EJEMPLO 1:
Pregunta: "¬øCu√°l es el mejor mes para visitar Machu Picchu?"
Respuesta: "El mejor periodo para visitar Machu Picchu es durante la estaci√≥n seca, 
de abril a octubre, siendo junio y julio los meses m√°s populares [Fuente: Gu√≠a_Cusco, 
p√°g. 45]. Sin embargo, estos meses tambi√©n son los m√°s concurridos. Si prefieres 
menos turistas, considera visitar en abril o septiembre, cuando el clima sigue siendo 
favorable [Fuente: Gu√≠a_Cusco, p√°g. 46].

IMPORTANTE: La temporada de lluvias (noviembre-marzo) puede dificultar el acceso, 
especialmente en febrero cuando cierran el Camino Inca por mantenimiento [Fuente: 
Gu√≠a_Cusco, p√°g. 47]."

EJEMPLO 2:
Pregunta: "Necesito informaci√≥n sobre restaurantes veganos en Arequipa"
Respuesta: "No tengo informaci√≥n suficiente en mis fuentes actuales sobre opciones 
espec√≠ficamente veganas en Arequipa. Sin embargo, puedo decirte que Arequipa es 
conocida por su gastronom√≠a tradicional que incluye platos como rocoto relleno y 
adobo arequipe√±o [Fuente: Gu√≠a_Gastronom√≠a_Peruana, p√°g. 78]. 

Te recomendar√≠a contactar directamente con restaurantes locales para consultar 
opciones veganas, o buscar en plataformas especializadas en turismo vegano."

AHORA RESPONDE A LA SIGUIENTE PREGUNTA:
"""
```

**Beneficios de Few-Shot:**
- Ense√±a el formato deseado
- Muestra manejo de incertidumbre
- Establece nivel de detalle esperado

##### **Dynamic Prompting basado en tipo de query**
```python
def get_prompt_template(query_type):
    """Selecciona prompt seg√∫n tipo de consulta"""
    
    if query_type == "recommendation":
        return """Eres un asesor tur√≠stico. El usuario busca recomendaciones.
        
        Estructura tu respuesta as√≠:
        1. **Recomendaci√≥n Principal**: [Lo m√°s adecuado]
        2. **Por qu√©**: [Justificaci√≥n con fuentes]
        3. **Alternativas**: [2-3 opciones adicionales]
        4. **Consideraciones**: [Clima, presupuesto, dificultad, etc.]
        
        Contexto: {context}
        Pregunta: {question}"""
    
    elif query_type == "factual":
        return """Eres una fuente de informaci√≥n factual sobre Per√∫.
        
        Proporciona:
        - Respuesta directa y concisa
        - Datos espec√≠ficos (fechas, precios, ubicaciones)
        - Fuentes de cada dato
        
        Contexto: {context}
        Pregunta: {question}"""
    
    elif query_type == "planning":
        return """Eres un planificador de viajes experto.
        
        Ayuda al usuario a crear un itinerario considerando:
        - D√≠as disponibles
        - Intereses (arqueolog√≠a, gastronom√≠a, naturaleza)
        - Log√≠stica (distancias, transporte)
        - Temporada y clima
        
        Presenta el plan en formato d√≠a por d√≠a.
        
        Contexto: {context}
        Pregunta: {question}"""
    
    # Default
    return SYSTEM_PROMPT
```

#### **3. Context Window Management**

##### **Token Budget Strategy**
```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.3")

def manage_context_window(
    query: str,
    retrieved_docs: list,
    max_tokens: int = 4096,
    reserved_for_response: int = 512
):
    """
    Gestiona el contexto para no exceder la ventana del modelo
    """
    query_tokens = len(tokenizer.encode(query))
    system_prompt_tokens = len(tokenizer.encode(SYSTEM_PROMPT))
    
    available_tokens = max_tokens - query_tokens - system_prompt_tokens - reserved_for_response
    
    # Priorizar documentos por score
    selected_docs = []
    current_tokens = 0
    
    for doc in retrieved_docs:
        doc_tokens = len(tokenizer.encode(doc.page_content))
        
        if current_tokens + doc_tokens <= available_tokens:
            selected_docs.append(doc)
            current_tokens += doc_tokens
        else:
            # Si queda espacio, truncar √∫ltimo documento
            if available_tokens - current_tokens > 100:  # Al menos 100 tokens
                truncated_content = tokenizer.decode(
                    tokenizer.encode(doc.page_content)[:(available_tokens - current_tokens)]
                )
                doc.page_content = truncated_content + "..."
                selected_docs.append(doc)
            break
    
    return selected_docs
```

**Rationale:**
- Modelos tienen l√≠mites de tokens
- Reservar espacio para respuesta
- Priorizar calidad sobre cantidad de contexto

#### **4. Response Post-Processing**

##### **Source Citation Extraction**
```python
def add_source_citations(response: str, source_docs: list) -> dict:
    """
    A√±ade citas de fuentes estructuradas
    """
    # Extraer metadata de documentos usados
    sources = []
    for doc in source_docs:
        source_info = {
            "document": doc.metadata.get("source", "Unknown"),
            "page": doc.metadata.get("page", "N/A"),
            "department": doc.metadata.get("department", "N/A"),
            "excerpt": doc.page_content[:200] + "..."
        }
        sources.append(source_info)
    
    return {
        "response": response,
        "sources": sources,
        "num_sources": len(sources),
        "confidence": calculate_confidence(response, source_docs)
    }

def calculate_confidence(response: str, source_docs: list) -> float:
    """
    Calcula score de confianza basado en:
    - Cantidad de fuentes
    - Relevancia de fuentes
    - Presencia de hedging language ("podr√≠a", "posiblemente")
    """
    confidence = 0.5  # Base
    
    # M√°s fuentes = m√°s confianza
    confidence += min(len(source_docs) * 0.1, 0.3)
    
    # Penalizar hedging language
    hedging_words = ["podr√≠a", "posiblemente", "quiz√°s", "probablemente"]
    for word in hedging_words:
        if word in response.lower():
            confidence -= 0.1
    
    # Bonus por citas espec√≠ficas
    if "[Fuente:" in response:
        confidence += 0.2
    
    return max(0.0, min(1.0, confidence))
```

##### **Hallucination Detection**
```python
from sentence_transformers import SentenceTransformer, util

def detect_hallucination(response: str, source_docs: list, threshold: float = 0.5) -> dict:
    """
    Detecta si la respuesta contiene informaci√≥n no soportada por las fuentes
    """
    model = SentenceTransformer('all-MiniLM-L6-v2')
    
    # Dividir respuesta en afirmaciones
    response_sentences = response.split('. ')
    
    # Concatenar todo el contexto
    context = " ".join([doc.page_content for doc in source_docs])
    
    # Calcular similarity de cada afirmaci√≥n con el contexto
    unsupported_claims = []
    
    for sentence in response_sentences:
        sentence_emb = model.encode(sentence, convert_to_tensor=True)
        context_emb = model.encode(context, convert_to_tensor=True)
        
        similarity = util.cos_sim(sentence_emb, context_emb).item()
        
        if similarity < threshold:
            unsupported_claims.append({
                "claim": sentence,
                "similarity": similarity
            })
    
    return {
        "is_grounded": len(unsupported_claims) == 0,
        "unsupported_claims": unsupported_claims,
        "grounding_score": 1.0 - (len(unsupported_claims) / len(response_sentences))
    }
```

---

## üé® STORYTELLING BEST PRACTICES

### Del "Storytelling with Data" (Cole Nussbaumer Knaflic)

#### **1. The Power of Context**

##### **Three-Minute Story Framework**
Para presentar el proyecto (README, demo, pitch):

```markdown
## PeruGuide AI: The Three-Minute Story

**SETUP (30 sec):** The Big Picture
- Per√∫: 2.5M+ tourists/year, incredibly diverse
- Problem: Information scattered across 100+ sources
- Gap: No intelligent, verified, personalized assistant

**CONFLICT (60 sec):** The Challenge
- Tourist: 20+ hours researching, still uncertain
- Agency: 50+ repetitive queries daily, opportunity cost
- Developer: Complex RAG systems, production challenges

**RESOLUTION (90 sec):** The Solution
- PeruGuide AI: 30+ official sources ‚Üí instant answers
- Architecture: Production-grade RAG with RAGAS evaluation
- Value: 95% time saved, 100% source citation, reproductible

[Visual: Before/After comparison]
[Demo: Live query ‚Üí instant response with sources]
```

**Aplicaci√≥n:**
- Presentaci√≥n de 3 minutos para entrevistas
- Demo script estructurado
- README structure: Problem ‚Üí Solution ‚Üí Results

#### **2. Eliminate Clutter**

##### **Documentation Principle: Signal vs. Noise**
```markdown
‚ùå BAD README:
```markdown
# My RAG Project

This project uses LangChain and FAISS and ChromaDB and implements RAG 
with embeddings using sentence transformers and has a FastAPI backend 
and Streamlit frontend and uses Docker and has evaluation with RAGAS 
and supports Spanish and English and...

[walls of text]
[no structure]
[everything is equally important = nothing stands out]
```

‚úÖ GOOD README:
```markdown
# üáµüá™ PeruGuide AI

> Transform 5,000+ pages of Peru tourism data into instant, verified recommendations

[Clear visual: Architecture diagram]

## ‚ö° Quick Start (< 2 min)
```bash
docker-compose up
open http://localhost:8501
```

## üéØ What Makes This Special
‚úÖ Production-grade RAG architecture
‚úÖ RAGAS-evaluated (Faithfulness: 0.87)
‚úÖ 30+ official data sources
‚úÖ 100% response citation

[Show, don't just tell]
```

**Principles Applied:**
- **Remove to improve**: Solo lo esencial en primera vista
- **Strategic use of color**: Emojis/badges para jerarqu√≠a visual
- **Progressive disclosure**: Links a detalles profundos
- **Visual hierarchy**: Headers, bullets, code blocks

#### **3. Focus Attention**

##### **README Structure con Attention Management**
```markdown
# [HERO IMAGE] ‚Üê Immediate visual impact

## [30-second pitch] ‚Üê Hook attention

## [Live Demo Link] ‚Üê Call-to-action

---

## Why This Matters ‚Üê Context
[Problem statement con storytelling]

## The Solution ‚Üê Value prop
[3 key features con √≠conos]

## See It In Action ‚Üê Proof
[GIF/video demo]

## Quick Start ‚Üê Remove friction
[One-command deployment]

---

[Detalles t√©cnicos en secciones colapsables o docs/ folder]
```

**Preattentive Attributes:**
- Size: Bigger = more important
- Color: Strategic use (green = success, red = warning, blue = info)
- Position: Top = most critical
- Enclosure: Boxes for grouping related items

#### **4. Think Like a Designer**

##### **Documentation Design System**
```markdown
Consistent Iconography:
üéØ = Goal/Objective
‚úÖ = Completed/Verified
üöÄ = Performance/Speed
üìä = Metrics/Evaluation
üèóÔ∏è = Architecture
üí° = Insight/Tip
‚ö†Ô∏è = Warning/Important
üìö = Documentation/Learning
üîß = Configuration/Setup
üé® = UI/UX

Code Block Conventions:
```python  # Python code
```bash    # Terminal commands
```yaml    # Configuration
```text    # Output/logs
```

Visual Hierarchy:
# H1: Project title only
## H2: Major sections
### H3: Subsections
#### H4: Details

Emphasis:
**Bold**: Key terms, important points
*Italic*: Subtle emphasis
`code`: Technical terms, commands
> Blockquote: Important callouts
```

##### **Notebook Design**
```python
# ‚ùå BAD: Wall of code without explanation
data = load_data()
chunks = split_text(data, 512, 64)
embeddings = get_embeddings(chunks)
db = create_db(embeddings)
retriever = db.as_retriever()
# ... 200 more lines

# ‚úÖ GOOD: Story-driven with explanations
"""
## üìö Step 1: Loading Our Knowledge Base

We have 30+ PDF guides about Peru tourism. Let's see what we're working with.
"""

data = load_data()
print(f"Loaded {len(data)} documents")
print(f"Total pages: {sum(len(doc.pages) for doc in data)}")

# Visualize distribution
plot_documents_by_department(data)

"""
### üí° Insight
Notice how Cusco and Lima have the most documentation? This makes sense 
as they're the top tourist destinations.

## üî™ Step 2: Chunking Strategy

We need to break documents into smaller pieces for effective retrieval.
But how small? Let's experiment...
"""

# Compare different chunk sizes
results = compare_chunk_sizes([256, 512, 1024])
plot_chunk_size_impact(results)

"""
### üìä Results
512 tokens with 64 overlap gives us the best balance:
- Preserves context (overlap)
- Fits in context window
- Maintains semantic coherence

Let's use that configuration:
"""

chunks = split_text(data, chunk_size=512, overlap=64)
```

**Storytelling Elements:**
- Headers as chapter titles
- Explanatory markdown cells
- Visualizations to show, not tell
- Progressive revelation
- Insights and conclusions

#### **5. Tell a Story with Data**

##### **Evaluation Results Presentation**
```markdown
# üìä Evaluation Results: The Journey to 0.87 Faithfulness

## The Baseline (Iteration 1)
We started with a simple RAG pipeline...

| Metric | Score | Status |
|--------|-------|--------|
| Faithfulness | 0.62 | üòû Too many hallucinations |
| Answer Relevancy | 0.71 | ü§î Decent but not great |
| Context Precision | 0.55 | üò± Retrieving wrong docs |

**What went wrong:**
- Naive chunking lost context at boundaries
- No reranking ‚Üí irrelevant docs in context
- Generic prompts ‚Üí model adding ungrounded info

## Iteration 2: Improved Chunking
Added overlap and preserved section context...

[Bar chart: Before vs After]

| Metric | Before | After | Œî |
|--------|--------|-------|---|
| Context Precision | 0.55 | 0.72 | +31% ‚úÖ |

## Iteration 3: Reranking
Added cross-encoder reranking...

[Line graph: Improvement trajectory]

| Metric | Before | After | Œî |
|--------|--------|-------|---|
| Context Precision | 0.72 | 0.84 | +17% ‚úÖ |
| Answer Relevancy | 0.71 | 0.78 | +10% ‚úÖ |

## Iteration 4: Prompt Engineering
Fine-tuned prompts with few-shot examples...

[Final comparison chart]

| Metric | Baseline | Final | Œî |
|--------|----------|-------|---|
| Faithfulness | 0.62 | 0.87 | +40% üéâ |
| Answer Relevancy | 0.71 | 0.82 | +15% ‚úÖ |
| Context Precision | 0.55 | 0.84 | +53% ‚≠ê |

## Key Learnings
1. **Context is king**: Better retrieval ‚Üí better generation
2. **Iterative wins**: Each small improvement compounds
3. **Measure everything**: Can't improve what you don't measure

[Call-to-action: See the code in notebooks/04_evaluation.ipynb]
```

**Narrative Arc:**
1. **Setup**: Baseline results (conflict)
2. **Rising Action**: Iterative improvements
3. **Climax**: Final results
4. **Resolution**: Key learnings
5. **Call-to-action**: Next steps

---

## üîç EVALUATION BEST PRACTICES (From RAGAS & Research)

### **Test Dataset Design**

#### **Diverse Query Types**
```python
test_queries = {
    "factual": [
        "¬øCu√°l es la altura de Machu Picchu?",
        "¬øEn qu√© departamento est√° el lago Titicaca?",
        "¬øCu√°ndo fue fundada la ciudad de Lima?"
    ],
    "recommendation": [
        "¬øQu√© lugares me recomiendas visitar en 7 d√≠as?",
        "¬øD√≥nde puedo probar la mejor comida peruana?",
        "¬øQu√© actividades de aventura hay en Cusco?"
    ],
    "planning": [
        "¬øC√≥mo organizar un viaje de 10 d√≠as incluyendo Machu Picchu?",
        "¬øQu√© ruta seguir para visitar la costa y la sierra?",
        "¬øCu√°l es el mejor itinerario para gastronom√≠a peruana?"
    ],
    "comparative": [
        "¬øQu√© diferencia hay entre Cusco y Arequipa?",
        "¬øEs mejor visitar Machu Picchu en junio o septiembre?",
        "¬øHuacachina o Paracas para turismo de aventura?"
    ],
    "edge_cases": [
        "¬øHay opciones veganas en la comida peruana?",  # Limited info
        "¬øQu√© hacer en Per√∫ con ni√±os peque√±os?",  # May not be covered
        "¬øCu√°l es el costo promedio diario de un viaje a Per√∫?"  # May not have specific numbers
    ]
}
```

#### **Ground Truth Creation**
```python
# Para cada query, crear ground truth manualmente:
ground_truth = {
    "query": "¬øCu√°l es la altura de Machu Picchu?",
    "expected_answer": "Machu Picchu est√° ubicado a 2,430 metros sobre el nivel del mar.",
    "expected_sources": ["Gu√≠a_Cusco.pdf", "page 34"],
    "category": "factual",
    "difficulty": "easy"
}
```

### **RAGAS Metrics in Practice**

```python
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall
)

# Preparar dataset
data = {
    "question": [...],
    "answer": [...],  # Generated by RAG
    "contexts": [...],  # Retrieved contexts
    "ground_truths": [...]  # Expected answers
}

# Evaluar
results = evaluate(
    dataset=data,
    metrics=[
        faithfulness,
        answer_relevancy,
        context_precision,
        context_recall
    ]
)

print(results)
```

**Interpretation:**
- **Faithfulness > 0.85**: Respuestas grounded en contexto
- **Answer Relevancy > 0.80**: Responde lo que se pregunt√≥
- **Context Precision > 0.75**: Contexto recuperado es relevante
- **Context Recall > 0.70**: Se recuper√≥ toda la info necesaria

---

## üöÄ DEPLOYMENT BEST PRACTICES

### **Docker Multi-Stage Build**
```dockerfile
# Stage 1: Builder
FROM python:3.11-slim as builder

WORKDIR /app

# Install build dependencies
RUN apt-get update && apt-get install -y \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir --user -r requirements.txt

# Stage 2: Runtime
FROM python:3.11-slim

WORKDIR /app

# Copy only necessary files from builder
COPY --from=builder /root/.local /root/.local
COPY src/ ./src/
COPY data/vector_stores/ ./data/vector_stores/

# Make sure scripts are in PATH
ENV PATH=/root/.local/bin:$PATH

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
  CMD python -c "import requests; requests.get('http://localhost:8000/health')"

# Run app
CMD ["uvicorn", "src.api.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

**Benefits:**
- Smaller final image (no build tools)
- Faster deployment
- Better security (minimal attack surface)

### **Docker Compose for Full Stack**
```yaml
version: '3.8'

services:
  api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - REDIS_HOST=redis
      - VECTOR_STORE_PATH=/data/vector_stores
    volumes:
      - ./data/vector_stores:/data/vector_stores:ro
    depends_on:
      - redis
    deploy:
      resources:
        limits:
          memory: 4G
  
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
  
  streamlit:
    build:
      context: .
      dockerfile: Dockerfile.streamlit
    ports:
      - "8501:8501"
    environment:
      - API_URL=http://api:8000
    depends_on:
      - api

volumes:
  redis_data:
```

---

## üìö CONCLUSI√ìN

Este documento sintetiza las mejores pr√°cticas extra√≠das de **9 libros y papers** de referencia, aplicadas espec√≠ficamente al proyecto **PeruGuide AI**.

**Key Takeaways:**
1. ‚úÖ **Engineering**: RAG no es solo "embeddings + LLM", requiere pipeline robusto
2. ‚úÖ **Evaluation**: M√©tricas objetivas son fundamentales para iteraci√≥n
3. ‚úÖ **Storytelling**: La narrativa convierte un proyecto t√©cnico en memorable
4. ‚úÖ **Production**: Gap teor√≠a-producci√≥n se cierra con arquitectura adecuada
5. ‚úÖ **Documentation**: Docs son tan importantes como el c√≥digo

**¬øSiguiente paso?**
Implementar estas pr√°cticas en el proyecto real, iterando y evaluando continuamente. üöÄ

---

*√öltima actualizaci√≥n: 23 Octubre 2025*
