# ğŸ““ Notebooks de Referencia - PeruGuide AI

**UbicaciÃ³n:** `analisis/notebook/`  
**PropÃ³sito:** DocumentaciÃ³n de experimentos y desarrollo acadÃ©mico previo  
**Fecha de archivo:** 23 de Octubre de 2025

---

## ğŸ“‹ Inventario de Notebooks

### 1. `00_analyze_reference_materials.ipynb`
**DescripciÃ³n:** AnÃ¡lisis exploratorio de materiales de referencia (libros tÃ©cnicos)

**Contenido esperado:**
- AnÃ¡lisis de 9 libros tÃ©cnicos sobre LLMs, RAG, embeddings
- ExtracciÃ³n de best practices de 2,959 pÃ¡ginas
- GeneraciÃ³n de `materials_analysis_comprehensive.json`

**Uso en Nivel 2:**
- âœ… Ya completado - resultados archivados en `analisis/`
- Los insights ya estÃ¡n documentados en `TECHNICAL_BEST_PRACTICES.md`
- No requiere re-ejecuciÃ³n

**Estado:** ğŸ“š Archivado (fase de anÃ¡lisis completada)

---

### 2. `MNA_NLP_actividad_chatbot_LLM_RAG (3).ipynb`
**DescripciÃ³n:** Notebook acadÃ©mico - Actividad de chatbot con LLM y RAG

**Contenido tÃ­pico:**
- ImplementaciÃ³n bÃ¡sica de RAG pipeline
- Pruebas de concepto con embeddings
- Experimentos con modelos LLM
- EvaluaciÃ³n bÃ¡sica de respuestas

**Lecciones para Nivel 2:**
- ğŸ” **Chunking:** Estrategia de divisiÃ³n de documentos
- ğŸ” **Retrieval:** ConfiguraciÃ³n de top_k y thresholds
- ğŸ” **Prompt Engineering:** Templates de prompts efectivos
- ğŸ” **Errores comunes:** Casos edge identificados

**RecomendaciÃ³n:**
- Revisar para extraer configuraciones que funcionaron bien
- Identificar parÃ¡metros Ã³ptimos (chunk_size, top_k, temperature)
- Documentar problemas encontrados y soluciones aplicadas

**AcciÃ³n sugerida:**
```bash
# Revisar notebook para extraer insights
jupyter notebook analisis/notebook/MNA_NLP_actividad_chatbot_LLM_RAG\ \(3\).ipynb

# O convertir a script para anÃ¡lisis
jupyter nbconvert --to python "analisis/notebook/MNA_NLP_actividad_chatbot_LLM_RAG (3).ipynb"
```

---

### 3. `NLP_LLM_con_RAG_y_VectorDatabase_FAISS_Chrome_Wikipedia.ipynb`
**DescripciÃ³n:** Experimento con RAG usando FAISS, ChromaDB y Wikipedia

**Contenido tÃ­pico:**
- ComparaciÃ³n FAISS vs ChromaDB
- IntegraciÃ³n con datos de Wikipedia
- Benchmarks de performance
- ConfiguraciÃ³n de vector stores

**Lecciones para Nivel 2:**
- ğŸ” **Vector Store:** ComparaciÃ³n FAISS vs Chroma (pros/cons)
- ğŸ” **Indexing:** Estrategias de indexaciÃ³n y persistencia
- ğŸ” **Benchmarking:** MÃ©tricas de latencia y throughput
- ğŸ” **Escalabilidad:** Pruebas con diferentes volÃºmenes de datos

**RecomendaciÃ³n:**
- Extraer configuraciones Ã³ptimas de FAISS/Chroma
- Documentar benchmarks para referencia
- Identificar lÃ­mites de escalabilidad observados

**AcciÃ³n sugerida:**
```bash
# Revisar notebook para configuraciones
jupyter notebook analisis/notebook/NLP_LLM_con_RAG_y_VectorDatabase_FAISS_Chrome_Wikipedia.ipynb

# Exportar resultados/benchmarks si existen
```

---

## ğŸ¯ CÃ³mo Usar estos Notebooks en Nivel 2

### **Paso 1: AuditorÃ­a de Experimentos**
1. Ejecutar cada notebook en el entorno Conda `peruguide-rag`
2. Documentar resultados y configuraciones exitosas
3. Extraer parÃ¡metros Ã³ptimos encontrados

### **Paso 2: ExtracciÃ³n de Conocimiento**
Crear un documento `LESSONS_LEARNED.md` con:
- âœ… **Configuraciones que funcionaron:**
  ```python
  chunk_size = 512  # Ã“ptimo segÃºn experimentos
  top_k = 5         # Balance precision/recall
  temperature = 0.3 # Respuestas consistentes
  ```
- âŒ **Errores a evitar:**
  - Chunks muy grandes (>1000 tokens) â†’ contexto irrelevante
  - top_k muy bajo (<3) â†’ pÃ©rdida de informaciÃ³n
  - temperature muy alta (>0.7) â†’ respuestas inconsistentes

### **Paso 3: MigraciÃ³n de CÃ³digo Ãštil**
Identificar snippets reutilizables:
```python
# Ejemplo: Si en notebook hay buen preprocessing
# Migrar a: src/data_pipeline/processors/cleaner.py

# Ejemplo: Si hay buena configuraciÃ³n de embeddings
# Migrar a: src/embedding_pipeline/models/sentence_transformer.py
```

### **Paso 4: Documentar Gaps**
Â¿QuÃ© falta en los notebooks que necesitamos en Nivel 2?
- Error handling robusto
- Logging estructurado
- Tests unitarios
- Monitoring de performance
- Deployment considerations

---

## ğŸ“Š ComparaciÃ³n: Notebooks vs Nivel 2

| Aspecto | Notebooks (AcadÃ©mico) | Nivel 2 (ProducciÃ³n) |
|---------|----------------------|----------------------|
| **Estructura** | Lineal, exploratorio | Modular, reutilizable |
| **Testing** | Manual, ad-hoc | Automatizado >75% coverage |
| **Error Handling** | BÃ¡sico/inexistente | Robusto con logging |
| **Documentation** | Markdown cells | Docstrings + MkDocs |
| **Performance** | No optimizado | Benchmarked, monitoreado |
| **Deployment** | No deployable | Docker + CI/CD ready |
| **Reproducibilidad** | Variable | 100% reproducible |

---

## ğŸ”„ Flujo de Trabajo Recomendado

### **Durante ImplementaciÃ³n Semana 1:**

```bash
# 1. Activar entorno
conda activate peruguide-rag

# 2. Revisar notebook relevante
jupyter notebook analisis/notebook/[NOTEBOOK_NAME].ipynb

# 3. Extraer configuraciÃ³n exitosa
# Ejemplo: chunk_size, embedding_model, etc.

# 4. Implementar en cÃ³digo profesional
# src/data_pipeline/...
# src/embedding_pipeline/...

# 5. Agregar tests
# tests/unit/test_[module].py

# 6. Documentar decisiÃ³n
# docs/architecture/decisions/ADR-001-chunking-strategy.md
```

### **Checklist de ExtracciÃ³n por Notebook:**

#### Para `MNA_NLP_actividad_chatbot_LLM_RAG (3).ipynb`:
- [ ] Extraer chunk_size Ã³ptimo
- [ ] Extraer prompt templates efectivos
- [ ] Identificar modelos LLM usados
- [ ] Documentar mÃ©tricas de evaluaciÃ³n
- [ ] Identificar casos edge/problemas

#### Para `NLP_LLM_con_RAG_y_VectorDatabase_FAISS_Chrome_Wikipedia.ipynb`:
- [ ] Extraer configuraciÃ³n FAISS exitosa
- [ ] Comparar FAISS vs Chroma (benchmarks)
- [ ] Documentar Ã­ndice Ã³ptimo (IndexFlatL2, etc.)
- [ ] Extraer estrategia de persistencia
- [ ] Identificar lÃ­mites de escalabilidad

---

## ğŸ’¡ Insights Clave a Buscar

### **1. HiperparÃ¡metros Ã“ptimos**
- `chunk_size`: Â¿CuÃ¡l dio mejores resultados?
- `chunk_overlap`: Â¿Necesario? Â¿CuÃ¡nto?
- `top_k`: Â¿CuÃ¡ntos chunks recuperar?
- `score_threshold`: Â¿Umbral mÃ­nimo de similaridad?

### **2. Modelos que Funcionaron**
- Embedding model usado y dimensiÃ³n
- LLM usado (local vs API)
- ConfiguraciÃ³n de temperature/max_tokens

### **3. Problemas Encontrados**
- Errores de encoding (UTF-8 vs Latin-1)
- PDFs mal formados
- Respuestas alucinadas del LLM
- Latencia inaceptable

### **4. Casos de Uso Validados**
- Â¿QuÃ© preguntas funcionaron bien?
- Â¿QuÃ© consultas fallaron?
- Â¿CuÃ¡l fue la precision/recall observada?

---

## ğŸ“ Template: LESSONS_LEARNED.md

DespuÃ©s de revisar notebooks, crear este documento:

```markdown
# Lessons Learned from Academic Notebooks

## Configuraciones Exitosas âœ…

### Chunking
- chunk_size: 512 tokens
- chunk_overlap: 64 tokens
- JustificaciÃ³n: Balance entre contexto y precisiÃ³n

### Embeddings
- Model: paraphrase-multilingual-mpnet-base-v2
- Dimension: 768
- Device: CPU (suficiente para <10k docs)

### Retrieval
- top_k: 5 chunks
- threshold: 0.7 cosine similarity
- search_type: similarity (MMR no mejorÃ³ resultados)

### LLM
- Model: Mistral-7B-Instruct v0.3
- temperature: 0.3 (respuestas consistentes)
- max_tokens: 512 (suficiente para respuestas completas)

## Problemas Identificados âŒ

1. **Encoding de PDFs:**
   - Problema: Algunos PDFs en Latin-1 causaban UnicodeDecodeError
   - SoluciÃ³n: Probar UTF-8, luego Latin-1 como fallback

2. **Chunks muy largos:**
   - Problema: chunks >1000 tokens incluÃ­an contexto irrelevante
   - SoluciÃ³n: chunk_size=512 como mÃ¡ximo

3. **Alucinaciones del LLM:**
   - Problema: LLM inventaba informaciÃ³n no presente en docs
   - SoluciÃ³n: Temperature baja (0.3) + prompt estricto

## MÃ©tricas Observadas ğŸ“Š

- Latencia promedio: ~2.5s por query
- Precision@5: ~0.82
- Context relevance: ~0.78

## Recomendaciones para Nivel 2 ğŸ¯

1. Implementar retry logic para PDFs con encoding issues
2. Agregar logging de latencia por componente
3. Implementar cache de embeddings
4. Agregar evaluaciÃ³n RAGAS desde inicio
```

---

## ğŸš€ PrÃ³ximos Pasos

1. **Esta semana:** Revisar notebooks y documentar insights
2. **Semana 1:** Usar configuraciones extraÃ­das en implementaciÃ³n
3. **Semana 2:** Comparar resultados Nivel 2 vs notebooks
4. **Semana 3:** Evaluar mejoras con RAGAS

---

## ğŸ“š Referencias

- Notebooks originales: `analisis/notebook/`
- AnÃ¡lisis de materiales: `analisis/materials_analysis_comprehensive.json`
- Best practices tÃ©cnicas: `TECHNICAL_BEST_PRACTICES.md`
- Planteamiento definitivo: `analisis/PLANTEAMIENTO_DEFINITIVO_CON_ANALISIS.md`

---

**Nota:** Estos notebooks representan el trabajo acadÃ©mico y experimental. El Nivel 2 toma los learnings y los convierte en cÃ³digo profesional, testeable y deployable.
