#!/usr/bin/env python3
"""
Quick Demo: PeruGuide RAG System
================================

Este script demuestra el sistema RAG completo usando un PDF pequeÃ±o de turismo.
NO requiere configuraciÃ³n de base de datos - usa solo FAISS en memoria.

Uso:
    python demo_quick.py
"""

import os
from pathlib import Path
from dotenv import load_dotenv

# Cargar variables de entorno
load_dotenv()

from src.data_pipeline.loaders.pdf_loader import PDFLoader
from src.data_pipeline.processors.cleaner import TextCleaner
from src.data_pipeline.chunkers.recursive_splitter import RecursiveCharacterTextSplitter
from src.embedding_pipeline.models.sentence_transformer import SentenceTransformerEmbedder
from src.vector_store.faiss_store import FaissVectorStore
from src.retrieval_pipeline.retrievers.semantic_retriever import SemanticRetriever
from src.llm import OpenAILLM, OpenAIConfig
from src.rag import AnswerGenerator


def main():
    """Demo rÃ¡pida del sistema RAG."""
    
    print("=" * 70)
    print("ğŸ‡µğŸ‡ª PERUGUIDE RAG SYSTEM - DEMO RÃPIDA")
    print("=" * 70)
    
    # ========================================================================
    # 1. CARGAR PDFS DE TURISMO
    # ========================================================================
    print("\nğŸ“„ [1/7] Cargando PDFs de turismo de PerÃº...")
    
    # Buscar todos los PDFs en data/raw/
    data_dir = Path("data/raw")
    pdf_files = list(data_dir.glob("*.pdf"))
    
    if not pdf_files:
        print("âŒ ERROR: No se encontraron PDFs en data/raw/")
        print("\nğŸ’¡ Copia algunos PDFs a la carpeta data/raw/ primero:")
        print("   Copy-Item 'deleteme\\Complementarios Peru\\*.pdf' data\\raw\\")
        return
    
    print(f"   âœ… Encontrados {len(pdf_files)} PDFs:")
    for pdf in pdf_files:
        size_mb = pdf.stat().st_size / (1024 * 1024)
        print(f"      â€¢ {pdf.name} ({size_mb:.2f} MB)")
    
    # Cargar todos los PDFs
    all_documents = []
    
    # Cargar todos los PDFs
    all_documents = []
    
    for pdf_path in pdf_files:
        print(f"   ğŸ“– Cargando: {pdf_path.name}...")
        try:
            # Crear loader para cada PDF
            loader = PDFLoader(source_dir=str(data_dir))
            doc = loader.load_single_pdf(pdf_path)
            
            if doc:
                # Convertir el Document a formato esperado (dict con text y metadata)
                doc_dict = {
                    "text": doc.text,
                    "metadata": {
                        **doc.metadata,
                        "source_file": pdf_path.name
                    }
                }
                all_documents.append(doc_dict)
                print(f"      âœ… PDF cargado ({len(doc.text)} caracteres)")
            else:
                print(f"      âš ï¸ No se pudo cargar el PDF")
        except Exception as e:
            print(f"      âš ï¸ Error: {e}")
    
    documents = all_documents
    print(f"\n   âœ… TOTAL: {len(documents)} PDFs cargados")
    
    # ========================================================================
    # 2. LIMPIAR TEXTO
    # ========================================================================
    print("\nğŸ§¹ [2/7] Limpiando y normalizando texto...")
    
    cleaner = TextCleaner()
    cleaned_docs = []
    for doc in documents:
        cleaned_text = cleaner.clean(doc["text"])
        cleaned_docs.append({
            "text": cleaned_text,
            "metadata": doc["metadata"]
        })
    
    print(f"   âœ… Texto limpiado de {len(cleaned_docs)} pÃ¡ginas")
    
    # ========================================================================
    # 3. DIVIDIR EN CHUNKS
    # ========================================================================
    print("\nâœ‚ï¸  [3/7] Dividiendo en chunks semÃ¡nticos...")
    
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=50
    )
    
    chunks = []
    for doc in cleaned_docs:
        doc_chunks = splitter.split_text(doc["text"])
        for i, chunk_text in enumerate(doc_chunks):
            chunks.append({
                "text": chunk_text,
                "metadata": {
                    **doc["metadata"],
                    "chunk_id": i
                }
            })
    
    print(f"   âœ… Creados {len(chunks)} chunks")
    
    # ========================================================================
    # 4. GENERAR EMBEDDINGS
    # ========================================================================
    print("\nğŸ§  [4/7] Generando embeddings con SentenceTransformer...")
    
    embedder = SentenceTransformerEmbedder(
        model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    )
    
    chunk_texts = [chunk["text"] for chunk in chunks]
    embeddings = embedder.encode_batch(chunk_texts)
    
    print(f"   âœ… Generados {len(embeddings)} embeddings de {embeddings[0].shape[0]} dimensiones")
    
    # ========================================================================
    # 5. CREAR VECTOR STORE (FAISS)
    # ========================================================================
    print("\nğŸ—„ï¸  [5/7] Creando vector store con FAISS...")
    
    vector_store = FaissVectorStore(dimension=embeddings[0].shape[0])
    
    # Preparar metadatos y IDs
    chunk_ids = [f"chunk_{i}" for i in range(len(chunks))]
    metadatas = [chunk["metadata"] for chunk in chunks]
    
    # Agregar embeddings al vector store
    vector_store.add(embeddings, chunk_ids, metadatas)
    
    print(f"   âœ… Vector store creado con {len(embeddings)} vectores")
    
    # ========================================================================
    # 6. CREAR RETRIEVER
    # ========================================================================
    print("\nğŸ” [6/7] Configurando sistema de recuperaciÃ³n semÃ¡ntica...")
    
    retriever = SemanticRetriever(
        embedder=embedder,
        vector_store=vector_store
    )
    
    print(f"   âœ… Retriever configurado")
    
    # ========================================================================
    # 7. CREAR GENERADOR DE RESPUESTAS
    # ========================================================================
    print("\nğŸ¤– [7/7] Configurando generador de respuestas (LLM)...")
    
    # Verificar que tenemos API key
    openai_key = os.getenv("OPENAI_API_KEY")
    if not openai_key:
        print("   âš ï¸  OPENAI_API_KEY no encontrada en .env")
        print("   â„¹ï¸  Modo de prueba: se mostrarÃ¡n los chunks recuperados sin generar respuesta")
        llm = None
    else:
        config = OpenAIConfig(
            api_key=openai_key,
            model="gpt-3.5-turbo",
            temperature=0.7,
            max_tokens=500
        )
        llm = OpenAILLM(config)
        print(f"   âœ… LLM configurado: OpenAI GPT-3.5-turbo")
    
    if llm:
        answer_generator = AnswerGenerator(llm=llm, retriever=retriever)
        print(f"   âœ… Answer Generator listo")
    else:
        answer_generator = None
    
    # ========================================================================
    # DEMO INTERACTIVA
    # ========================================================================
    print("\n" + "=" * 70)
    print("âœ… SISTEMA RAG LISTO - PRUEBAS INTERACTIVAS")
    print("=" * 70)
    
    # Queries de ejemplo
    example_queries = [
        "Â¿CuÃ¡les son los principales atractivos turÃ­sticos de Cusco?",
        "Â¿QuÃ© platos tÃ­picos son famosos en la gastronomÃ­a peruana?",
        "HÃ¡blame sobre el turismo en PerÃº y sus destinos principales",
        "Â¿QuÃ© actividades puedo hacer en Cusco?",
        "Â¿CuÃ¡les son las caracterÃ­sticas de la cocina peruana?"
    ]
    
    print("\nğŸ“ QUERIES DE EJEMPLO:")
    for i, query in enumerate(example_queries, 1):
        print(f"   {i}. {query}")
    
    print("\n" + "-" * 70)
    
    while True:
        print("\nğŸ’¬ Ingresa tu pregunta sobre turismo en PerÃº (o 'q' para salir):")
        print("   (o ingresa un nÃºmero 1-5 para usar un ejemplo, o Enter para ejemplo 1)")
        
        user_input = input(">>> ").strip()
        
        if user_input.lower() == 'q':
            print("\nğŸ‘‹ Â¡Hasta luego!")
            break
        
        # Usar query de ejemplo si estÃ¡ vacÃ­o
        if not user_input:
            query = example_queries[0]
            print(f"   Usando ejemplo: {query}")
        elif user_input.isdigit() and 1 <= int(user_input) <= len(example_queries):
            query = example_queries[int(user_input) - 1]
            print(f"   Usando ejemplo {user_input}: {query}")
        else:
            query = user_input
        
        print(f"\nğŸ” Buscando informaciÃ³n relevante...")
        
        # Recuperar contexto
        retrieved_docs = retriever.retrieve(query, k=5)
        
        print(f"âœ… Encontrados {len(retrieved_docs)} chunks relevantes:")
        print("-" * 70)
        
        for i, doc in enumerate(retrieved_docs, 1):
            print(f"\nğŸ“„ Chunk {i} (score: {doc['score']:.4f}):")
            print(f"   PÃ¡gina: {doc['metadata'].get('page', 'N/A')}")
            print(f"   Texto: {doc['text'][:200]}...")
        
        if llm and answer_generator:
            print(f"\nğŸ¤– Generando respuesta con LLM...")
            
            try:
                response = answer_generator.generate_answer(query)
                
                print("\n" + "=" * 70)
                print("ğŸ’¡ RESPUESTA:")
                print("=" * 70)
                print(response.answer)
                
                if response.citations:
                    print("\nğŸ“š FUENTES:")
                    for i, citation in enumerate(response.citations, 1):
                        print(f"   [{i}] PÃ¡gina {citation.get('page', 'N/A')}")
                
                print(f"\nâ±ï¸  Tiempo: {response.metadata.get('generation_time', 'N/A')}s")
                print(f"ğŸ”¢ Tokens: {response.metadata.get('total_tokens', 'N/A')}")
                
            except Exception as e:
                print(f"\nâŒ Error generando respuesta: {e}")
                print("   â„¹ï¸  Los chunks recuperados se muestran arriba")
        else:
            print("\nğŸ’¡ Para generar respuestas con LLM, configura OPENAI_API_KEY en .env")
        
        print("\n" + "-" * 70)
    
    print("\n" + "=" * 70)
    print("ğŸ“Š ESTADÃSTICAS DE LA DEMO:")
    print("=" * 70)
    print(f"   â€¢ PDFs procesados: {len(pdf_files)}")
    for pdf in pdf_files:
        print(f"      - {pdf.name}")
    print(f"   â€¢ PÃ¡ginas totales: {len(documents)}")
    print(f"   â€¢ Chunks creados: {len(chunks)}")
    print(f"   â€¢ Embeddings: {len(embeddings)}")
    print(f"   â€¢ DimensiÃ³n vectorial: {embeddings[0].shape[0]}")
    print(f"   â€¢ Vector store: FAISS")
    print(f"   â€¢ Modelo embeddings: paraphrase-multilingual-MiniLM-L12-v2")
    if llm:
        print(f"   â€¢ LLM: {llm.__class__.__name__}")
    print("=" * 70)


if __name__ == "__main__":
    main()
