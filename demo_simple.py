#!/usr/bin/env python3
"""
Demo RÃ¡pido - Sistema RAG PeruGuide
====================================

DemostraciÃ³n del flujo completo RAG sin necesidad de PDFs grandes.
Usa texto de ejemplo para mostrar todas las etapas del pipeline.
"""

import os
from pathlib import Path
from dotenv import load_dotenv

load_dotenv()

print("=" * 70)
print("ðŸ‡µðŸ‡ª DEMO RÃPIDO - SISTEMA RAG PERUGUIDE")
print("=" * 70)

# ============================================================================
# TEXTO DE EJEMPLO (simula contenido de PDFs de turismo)
# ============================================================================
print("\nðŸ“„ [1/7] Preparando datos de ejemplo...")

sample_texts = [
    {
        "text": """Machu Picchu es una antigua ciudad inca ubicada en las montaÃ±as de los Andes 
        en PerÃº. Fue construida en el siglo XV y es considerada una de las Nuevas Siete Maravillas 
        del Mundo Moderno. La ciudadela se encuentra a 2,430 metros sobre el nivel del mar y ofrece 
        vistas espectaculares del valle del rÃ­o Urubamba. Para llegar se puede tomar el tren desde 
        Cusco o hacer el famoso Camino Inca que dura 4 dÃ­as.""",
        "metadata": {"source": "Cusco_Turismo", "page": 1, "topic": "machu_picchu"}
    },
    {
        "text": """El ceviche es el plato bandera de la gastronomÃ­a peruana. Se prepara con pescado 
        fresco marinado en jugo de limÃ³n, ajÃ­ limo, cebolla roja y cilantro. Es un plato refrescante 
        que combina perfectamente los sabores cÃ­tricos con el picante caracterÃ­stico de la cocina peruana. 
        Los mejores ceviches se encuentran en la costa, especialmente en Lima y el norte del paÃ­s.""",
        "metadata": {"source": "Gastronomia_Peruana", "page": 1, "topic": "ceviche"}
    },
    {
        "text": """Lima, la capital de PerÃº, es conocida como la 'Ciudad de los Reyes'. Fue fundada 
        en 1535 por Francisco Pizarro. El centro histÃ³rico de Lima es Patrimonio de la Humanidad 
        y alberga magnÃ­ficas iglesias coloniales, museos y la famosa Plaza de Armas. Destacan el 
        Convento de San Francisco con sus catacumbas, la Catedral de Lima y el Palacio de Gobierno.""",
        "metadata": {"source": "Lima_Info", "page": 1, "topic": "lima_capital"}
    },
    {
        "text": """El Valle Sagrado de los Incas se encuentra cerca de Cusco y es una regiÃ³n rica 
        en sitios arqueolÃ³gicos. Incluye pueblos como Pisac con su mercado artesanal y fortaleza inca, 
        Ollantaytambo con sus terrazas agrÃ­colas impresionantes, y Chinchero conocido por sus textiles 
        tradicionales. Es un destino imperdible para los amantes de la historia y la cultura andina.""",
        "metadata": {"source": "Cusco_Turismo", "page": 2, "topic": "valle_sagrado"}
    },
    {
        "text": """La papa es un ingrediente fundamental en la cocina peruana. PerÃº tiene mÃ¡s de 
        3,000 variedades de papa, cada una con sabores y texturas Ãºnicos. La causa limeÃ±a es un 
        plato frÃ­o elaborado con papa amarilla, limÃ³n y ajÃ­ amarillo. El ajÃ­ de gallina combina 
        papa con una salsa cremosa de ajÃ­ amarillo y pollo deshilachado. Ambos son Ã­conos de la 
        gastronomÃ­a criolla peruana.""",
        "metadata": {"source": "Gastronomia_Peruana", "page": 2, "topic": "papa_cocina"}
    },
    {
        "text": """El Museo Larco en Lima es uno de los museos mÃ¡s importantes del paÃ­s. Alberga 
        una impresionante colecciÃ³n de arte precolombino con mÃ¡s de 45,000 piezas arqueolÃ³gicas. 
        Destaca su sala de cerÃ¡mica erÃ³tica y su colecciÃ³n de oro y plata. El museo estÃ¡ ubicado 
        en una mansiÃ³n virreinal del siglo XVIII rodeada de hermosos jardines.""",
        "metadata": {"source": "Lima_Info", "page": 2, "topic": "museos_lima"}
    },
    {
        "text": """Arequipa, conocida como la Ciudad Blanca por sus construcciones de sillar volcÃ¡nico, 
        es la segunda ciudad mÃ¡s importante de PerÃº. Su centro histÃ³rico es Patrimonio de la Humanidad. 
        El Monasterio de Santa Catalina es una ciudadela dentro de la ciudad con calles coloridas. 
        Cerca estÃ¡ el CaÃ±Ã³n del Colca, uno de los caÃ±ones mÃ¡s profundos del mundo donde se pueden 
        observar cÃ³ndores en vuelo.""",
        "metadata": {"source": "Arequipa_Turismo", "page": 1, "topic": "arequipa"}
    },
    {
        "text": """El lomo saltado es un plato emblemÃ¡tico que fusiona la cocina china con la peruana. 
        Combina tiras de carne de res salteadas con cebolla, tomate, ajÃ­ amarillo y papas fritas. 
        Se sirve acompaÃ±ado de arroz. Este plato es resultado de la inmigraciÃ³n china al PerÃº en 
        el siglo XIX y representa la fusiÃ³n culinaria conocida como cocina chifa.""",
        "metadata": {"source": "Gastronomia_Peruana", "page": 3, "topic": "lomo_saltado"}
    }
]

print(f"   âœ… {len(sample_texts)} documentos de ejemplo preparados")

# ============================================================================
# [2/7] LIMPIAR TEXTO
# ============================================================================
print("\nðŸ§¹ [2/7] Limpiando y normalizando texto...")

from src.data_pipeline.processors.cleaner import TextCleaner

cleaner = TextCleaner()
cleaned_docs = []

for doc in sample_texts:
    cleaned_text = cleaner.clean(doc["text"])
    cleaned_docs.append({
        "text": cleaned_text,
        "metadata": doc["metadata"]
    })

print(f"   âœ… {len(cleaned_docs)} documentos limpiados")

# ============================================================================
# [3/7] DIVIDIR EN CHUNKS
# ============================================================================
print("\nâœ‚ï¸  [3/7] Dividiendo en chunks semÃ¡nticos...")

from src.data_pipeline.chunkers.recursive_splitter import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=200,
    chunk_overlap=20
)

chunks = []
for doc in cleaned_docs:
    doc_chunks = splitter.split_text(doc["text"])
    for i, chunk_text in enumerate(doc_chunks):
        chunks.append({
            "text": chunk_text,
            "metadata": {
                **doc["metadata"],
                "chunk_id": i
            }
        })

print(f"   âœ… Creados {len(chunks)} chunks")

# ============================================================================
# [4/7] GENERAR EMBEDDINGS
# ============================================================================
print("\nðŸ§  [4/7] Generando embeddings con SentenceTransformer...")
print("   â³ Descargando modelo (primera vez puede tardar)...")

from src.embedding_pipeline.models.sentence_transformer import SentenceTransformerEmbedder

# Usar modelo MiniLM que genera embeddings de 384 dimensiones
embedder = SentenceTransformerEmbedder(
    model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
    dimension=384  # Especificar dimensiÃ³n correcta para este modelo
)

chunk_texts = [chunk["text"] for chunk in chunks]
embeddings = embedder.encode_batch(chunk_texts)

# Usar la dimensiÃ³n real de los embeddings generados
actual_dimension = embeddings[0].shape[0]
print(f"   âœ… Generados {len(embeddings)} embeddings de {actual_dimension} dimensiones")

# ============================================================================
# [5/7] CREAR VECTOR STORE (FAISS)
# ============================================================================
print("\nðŸ—„ï¸  [5/7] Creando vector store con FAISS...")

from src.vector_store.faiss_store import FaissVectorStore

# Usar la dimensiÃ³n real de los embeddings
vector_store = FaissVectorStore(dimension=actual_dimension)

chunk_ids = [f"chunk_{i}" for i in range(len(chunks))]
metadatas = [chunk["metadata"] for chunk in chunks]

# Agregar el texto a los metadatos para poder recuperarlo despuÃ©s
for i, chunk in enumerate(chunks):
    metadatas[i]["text"] = chunk["text"]

vector_store.add(embeddings, chunk_ids, metadatas)

print(f"   âœ… Vector store creado con {len(embeddings)} vectores")

# ============================================================================
# [6/7] CREAR RETRIEVER
# ============================================================================
print("\nðŸ” [6/7] Configurando sistema de recuperaciÃ³n semÃ¡ntica...")

from src.retrieval_pipeline.retrievers.semantic_retriever import SemanticRetriever

retriever = SemanticRetriever(
    embedder=embedder,
    vector_store=vector_store
)

print(f"   âœ… Retriever configurado")

# ============================================================================
# [7/7] CREAR GENERADOR DE RESPUESTAS
# ============================================================================
print("\nðŸ¤– [7/7] Configurando generador de respuestas (LLM)...")

openai_key = os.getenv("OPENAI_API_KEY")
if not openai_key:
    print("   âš ï¸  OPENAI_API_KEY no encontrada en .env")
    print("   â„¹ï¸  Modo de prueba: solo se mostrarÃ¡n chunks recuperados")
    llm = None
    answer_generator = None
else:
    from src.llm import OpenAILLM, OpenAIConfig
    from src.rag import AnswerGenerator
    
    config = OpenAIConfig(
        api_key=openai_key,
        model="gpt-3.5-turbo",
        temperature=0.7,
        max_tokens=300
    )
    llm = OpenAILLM(config)
    answer_generator = AnswerGenerator(llm=llm, retriever=retriever)
    print(f"   âœ… LLM configurado: OpenAI GPT-3.5-turbo")

# ============================================================================
# DEMO INTERACTIVA
# ============================================================================
print("\n" + "=" * 70)
print("âœ… SISTEMA RAG LISTO - DEMO INTERACTIVA")
print("=" * 70)

example_queries = [
    "Â¿QuÃ© es Machu Picchu y dÃ³nde estÃ¡ ubicado?",
    "Â¿CÃ³mo se prepara el ceviche peruano?",
    "Â¿QuÃ© lugares turÃ­sticos hay en el Valle Sagrado?",
    "CuÃ©ntame sobre la gastronomÃ­a peruana",
    "Â¿QuÃ© atractivos tiene Lima?"
]

print("\nðŸ“ QUERIES DE EJEMPLO:")
for i, query in enumerate(example_queries, 1):
    print(f"   {i}. {query}")

print("\n" + "-" * 70)

while True:
    print("\nðŸ’¬ Ingresa tu pregunta sobre turismo en PerÃº (o 'q' para salir):")
    print("   (o nÃºmero 1-5 para usar ejemplo, o Enter para ejemplo 1)")
    
    user_input = input(">>> ").strip()
    
    if user_input.lower() == 'q':
        print("\nðŸ‘‹ Â¡Hasta luego!")
        break
    
    if not user_input:
        query = example_queries[0]
        print(f"   Usando ejemplo: {query}")
    elif user_input.isdigit() and 1 <= int(user_input) <= len(example_queries):
        query = example_queries[int(user_input) - 1]
        print(f"   Usando ejemplo {user_input}: {query}")
    else:
        query = user_input
    
    print(f"\nðŸ” Buscando informaciÃ³n relevante...")
    
    retrieved_docs = retriever.retrieve(query, k=3)
    
    print(f"âœ… Encontrados {len(retrieved_docs)} chunks relevantes:")
    print("-" * 70)
    
    for i, doc in enumerate(retrieved_docs, 1):
        print(f"\nðŸ“„ Chunk {i} (score: {doc['score']:.4f}):")
        print(f"   Fuente: {doc['metadata'].get('source', 'N/A')}")
        # El texto estÃ¡ en metadata
        text = doc['metadata'].get('text', 'N/A')
        print(f"   Texto: {text[:150]}...")
    
    if answer_generator:
        print(f"\nðŸ¤– Generando respuesta con LLM...")
        
        try:
            response = answer_generator.generate_answer(query)
            
            print("\n" + "=" * 70)
            print("ðŸ’¡ RESPUESTA:")
            print("=" * 70)
            print(response.answer)
            
            if response.citations:
                print("\nðŸ“š FUENTES:")
                for i, citation in enumerate(response.citations, 1):
                    print(f"   [{i}] {citation.get('source', 'N/A')}")
            
        except Exception as e:
            print(f"\nâŒ Error generando respuesta: {e}")
            print("   â„¹ï¸  Los chunks recuperados se muestran arriba")
    else:
        print("\nðŸ’¡ Para generar respuestas con LLM, configura OPENAI_API_KEY en .env")
    
    print("\n" + "-" * 70)

print("\n" + "=" * 70)
print("ðŸ“Š ESTADÃSTICAS DE LA DEMO:")
print("=" * 70)
print(f"   â€¢ Documentos de ejemplo: {len(sample_texts)}")
print(f"   â€¢ Chunks creados: {len(chunks)}")
print(f"   â€¢ Embeddings: {len(embeddings)}")
print(f"   â€¢ DimensiÃ³n vectorial: {embeddings[0].shape[0]}")
print(f"   â€¢ Vector store: FAISS")
print(f"   â€¢ Modelo embeddings: paraphrase-multilingual-MiniLM-L12-v2")
if llm:
    print(f"   â€¢ LLM: OpenAI GPT-3.5-turbo")
print("=" * 70)
print("\nâœ¨ Demo completada exitosamente!")
