#!/usr/bin/env python3
"""
Demo R√°pido - Sistema RAG PeruGuide
====================================

Demostraci√≥n del flujo completo RAG sin necesidad de PDFs grandes.
Usa texto de ejemplo para mostrar todas las etapas del pipeline.
"""

import os
from pathlib import Path
from dotenv import load_dotenv

load_dotenv()

print("=" * 70)
print("üáµüá™ DEMO R√ÅPIDO - SISTEMA RAG PERUGUIDE")
print("=" * 70)

# ============================================================================
# TEXTO DE EJEMPLO (simula contenido de PDFs de turismo)
# ============================================================================
print("\nüìÑ [1/7] Preparando datos de ejemplo...")

sample_texts = [
    {
        "text": """Machu Picchu es una antigua ciudad inca ubicada en las monta√±as de los Andes 
        en Per√∫. Fue construida en el siglo XV y es considerada una de las Nuevas Siete Maravillas 
        del Mundo Moderno. La ciudadela se encuentra a 2,430 metros sobre el nivel del mar y ofrece 
        vistas espectaculares del valle del r√≠o Urubamba. Para llegar se puede tomar el tren desde 
        Cusco o hacer el famoso Camino Inca que dura 4 d√≠as.""",
        "metadata": {"source": "Cusco_Turismo", "page": 1, "topic": "machu_picchu"}
    },
    {
        "text": """El ceviche es el plato bandera de la gastronom√≠a peruana. Se prepara con pescado 
        fresco marinado en jugo de lim√≥n, aj√≠ limo, cebolla roja y cilantro. Es un plato refrescante 
        que combina perfectamente los sabores c√≠tricos con el picante caracter√≠stico de la cocina peruana. 
        Los mejores ceviches se encuentran en la costa, especialmente en Lima y el norte del pa√≠s.""",
        "metadata": {"source": "Gastronomia_Peruana", "page": 1, "topic": "ceviche"}
    },
    {
        "text": """Lima, la capital de Per√∫, es conocida como la 'Ciudad de los Reyes'. Fue fundada 
        en 1535 por Francisco Pizarro. El centro hist√≥rico de Lima es Patrimonio de la Humanidad 
        y alberga magn√≠ficas iglesias coloniales, museos y la famosa Plaza de Armas. Destacan el 
        Convento de San Francisco con sus catacumbas, la Catedral de Lima y el Palacio de Gobierno.""",
        "metadata": {"source": "Lima_Info", "page": 1, "topic": "lima_capital"}
    },
    {
        "text": """El Valle Sagrado de los Incas se encuentra cerca de Cusco y es una regi√≥n rica 
        en sitios arqueol√≥gicos. Incluye pueblos como Pisac con su mercado artesanal y fortaleza inca, 
        Ollantaytambo con sus terrazas agr√≠colas impresionantes, y Chinchero conocido por sus textiles 
        tradicionales. Es un destino imperdible para los amantes de la historia y la cultura andina.""",
        "metadata": {"source": "Cusco_Turismo", "page": 2, "topic": "valle_sagrado"}
    },
    {
        "text": """La papa es un ingrediente fundamental en la cocina peruana. Per√∫ tiene m√°s de 
        3,000 variedades de papa, cada una con sabores y texturas √∫nicos. La causa lime√±a es un 
        plato fr√≠o elaborado con papa amarilla, lim√≥n y aj√≠ amarillo. El aj√≠ de gallina combina 
        papa con una salsa cremosa de aj√≠ amarillo y pollo deshilachado. Ambos son √≠conos de la 
        gastronom√≠a criolla peruana.""",
        "metadata": {"source": "Gastronomia_Peruana", "page": 2, "topic": "papa_cocina"}
    },
    {
        "text": """El Museo Larco en Lima es uno de los museos m√°s importantes del pa√≠s. Alberga 
        una impresionante colecci√≥n de arte precolombino con m√°s de 45,000 piezas arqueol√≥gicas. 
        Destaca su sala de cer√°mica er√≥tica y su colecci√≥n de oro y plata. El museo est√° ubicado 
        en una mansi√≥n virreinal del siglo XVIII rodeada de hermosos jardines.""",
        "metadata": {"source": "Lima_Info", "page": 2, "topic": "museos_lima"}
    },
    {
        "text": """Arequipa, conocida como la Ciudad Blanca por sus construcciones de sillar volc√°nico, 
        es la segunda ciudad m√°s importante de Per√∫. Su centro hist√≥rico es Patrimonio de la Humanidad. 
        El Monasterio de Santa Catalina es una ciudadela dentro de la ciudad con calles coloridas. 
        Cerca est√° el Ca√±√≥n del Colca, uno de los ca√±ones m√°s profundos del mundo donde se pueden 
        observar c√≥ndores en vuelo.""",
        "metadata": {"source": "Arequipa_Turismo", "page": 1, "topic": "arequipa"}
    },
    {
        "text": """El lomo saltado es un plato emblem√°tico que fusiona la cocina china con la peruana. 
        Combina tiras de carne de res salteadas con cebolla, tomate, aj√≠ amarillo y papas fritas. 
        Se sirve acompa√±ado de arroz. Este plato es resultado de la inmigraci√≥n china al Per√∫ en 
        el siglo XIX y representa la fusi√≥n culinaria conocida como cocina chifa.""",
        "metadata": {"source": "Gastronomia_Peruana", "page": 3, "topic": "lomo_saltado"}
    }
]

print(f"   ‚úÖ {len(sample_texts)} documentos de ejemplo preparados")

# ============================================================================
# [2/7] LIMPIAR TEXTO
# ============================================================================
print("\nüßπ [2/7] Limpiando y normalizando texto...")

from src.data_pipeline.processors.cleaner import TextCleaner

cleaner = TextCleaner()
cleaned_docs = []

for doc in sample_texts:
    cleaned_text = cleaner.clean(doc["text"])
    cleaned_docs.append({
        "text": cleaned_text,
        "metadata": doc["metadata"]
    })

print(f"   ‚úÖ {len(cleaned_docs)} documentos limpiados")

# ============================================================================
# [3/7] DIVIDIR EN CHUNKS
# ============================================================================
print("\n‚úÇÔ∏è  [3/7] Dividiendo en chunks sem√°nticos...")

from src.data_pipeline.chunkers.recursive_splitter import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=200,
    chunk_overlap=20
)

chunks = []
for doc in cleaned_docs:
    doc_chunks = splitter.split_text(doc["text"])
    for i, chunk_text in enumerate(doc_chunks):
        chunks.append({
            "text": chunk_text,
            "metadata": {
                **doc["metadata"],
                "chunk_id": i
            }
        })

print(f"   ‚úÖ Creados {len(chunks)} chunks")

# ============================================================================
# [4/7] GENERAR EMBEDDINGS
# ============================================================================
print("\nüß† [4/7] Generando embeddings con SentenceTransformer...")
print("   ‚è≥ Descargando modelo (primera vez puede tardar)...")

from src.embedding_pipeline.models.sentence_transformer import SentenceTransformerEmbedder

# Usar modelo MiniLM que genera embeddings de 384 dimensiones
embedder = SentenceTransformerEmbedder(
    model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
    dimension=384  # Especificar dimensi√≥n correcta para este modelo
)

chunk_texts = [chunk["text"] for chunk in chunks]
embeddings = embedder.encode_batch(chunk_texts)

# Usar la dimensi√≥n real de los embeddings generados
actual_dimension = embeddings[0].shape[0]
print(f"   ‚úÖ Generados {len(embeddings)} embeddings de {actual_dimension} dimensiones")

# ============================================================================
# [5/7] CREAR VECTOR STORE (FAISS)
# ============================================================================
print("\nüóÑÔ∏è  [5/7] Creando vector store con FAISS...")

from src.vector_store.faiss_store import FaissVectorStore

# Usar la dimensi√≥n real de los embeddings
vector_store = FaissVectorStore(dimension=actual_dimension)

chunk_ids = [f"chunk_{i}" for i in range(len(chunks))]
metadatas = [chunk["metadata"] for chunk in chunks]

# Agregar el texto a los metadatos para poder recuperarlo despu√©s
for i, chunk in enumerate(chunks):
    metadatas[i]["text"] = chunk["text"]

vector_store.add(embeddings, chunk_ids, metadatas)

print(f"   ‚úÖ Vector store creado con {len(embeddings)} vectores")

# ============================================================================
# [6/7] CREAR RETRIEVER
# ============================================================================
print("\nüîç [6/7] Configurando sistema de recuperaci√≥n sem√°ntica...")

from src.retrieval_pipeline.retrievers.semantic_retriever import SemanticRetriever

retriever = SemanticRetriever(
    embedder=embedder,
    vector_store=vector_store
)

print(f"   ‚úÖ Retriever configurado")

# ============================================================================
# [7/7] CREAR GENERADOR DE RESPUESTAS
# ============================================================================
print("\nü§ñ [7/7] Configurando generador de respuestas (LLM)...")

openai_key = os.getenv("OPENAI_API_KEY")
if not openai_key:
    print("   ‚ö†Ô∏è  OPENAI_API_KEY no encontrada en .env")
    print("   ‚ÑπÔ∏è  Modo de prueba: solo se mostrar√°n chunks recuperados")
    llm = None
    answer_generator = None
else:
    from src.llm import OpenAILLM, OpenAIConfig
    from src.rag import AnswerGenerator
    
    config = OpenAIConfig(
        api_key=openai_key,
        model="gpt-3.5-turbo",
        temperature=0.7,
        max_tokens=300
    )
    llm = OpenAILLM(config)
    answer_generator = AnswerGenerator(llm=llm, retriever=retriever)
    print(f"   ‚úÖ LLM configurado: OpenAI GPT-3.5-turbo")

# ============================================================================
# DEMO INTERACTIVA
# ============================================================================
print("\n" + "=" * 70)
print("‚úÖ SISTEMA RAG LISTO - DEMO INTERACTIVA")
print("=" * 70)

example_queries = [
    "¬øQu√© es Machu Picchu y d√≥nde est√° ubicado?",
    "¬øC√≥mo se prepara el ceviche peruano?",
    "¬øQu√© lugares tur√≠sticos hay en el Valle Sagrado?",
    "Cu√©ntame sobre la gastronom√≠a peruana",
    "¬øQu√© atractivos tiene Lima?"
]

print("\nüìù QUERIES DE EJEMPLO:")
for i, query in enumerate(example_queries, 1):
    print(f"   {i}. {query}")

print("\n" + "-" * 70)

while True:
    print("\nüí¨ Ingresa tu pregunta sobre turismo en Per√∫ (o 'q' para salir):")
    print("   (o n√∫mero 1-5 para usar ejemplo, o Enter para ejemplo 1)")
    
    user_input = input(">>> ").strip()
    
    if user_input.lower() == 'q':
        print("\nüëã ¬°Hasta luego!")
        break
    
    if not user_input:
        query = example_queries[0]
        print(f"   Usando ejemplo: {query}")
    elif user_input.isdigit() and 1 <= int(user_input) <= len(example_queries):
        query = example_queries[int(user_input) - 1]
        print(f"   Usando ejemplo {user_input}: {query}")
    else:
        query = user_input
    
    print(f"\nüîç Buscando informaci√≥n relevante...")
    
    retrieved_docs = retriever.retrieve(query, k=3)
    
    print(f"‚úÖ Encontrados {len(retrieved_docs)} chunks relevantes:")
    print("-" * 70)
    
    for i, doc in enumerate(retrieved_docs, 1):
        print(f"\nüìÑ Chunk {i} (score: {doc['score']:.4f}):")
        print(f"   Fuente: {doc['metadata'].get('source', 'N/A')}")
        # El texto est√° en metadata
        text = doc['metadata'].get('text', 'N/A')
        print(f"   Texto: {text[:150]}...")
    
    if answer_generator:
        print(f"\nü§ñ Generando respuesta con LLM...")
        
        try:
            response = answer_generator.generate_answer(query)
            
            print("\n" + "=" * 70)
            print("üí° RESPUESTA:")
            print("=" * 70)
            print(response.answer)
            
            if response.citations:
                print("\nüìö FUENTES:")
                for i, citation in enumerate(response.citations, 1):
                    print(f"   [{i}] {citation.get('source', 'N/A')}")
            
        except Exception as e:
            print(f"\n‚ùå Error generando respuesta: {e}")
            print("   ‚ÑπÔ∏è  Los chunks recuperados se muestran arriba")
    else:
        print("\nüí° Para generar respuestas con LLM, configura OPENAI_API_KEY en .env")
    
    print("\n" + "-" * 70)

print("\n" + "=" * 70)
print("üìä ESTAD√çSTICAS DE LA DEMO:")
print("=" * 70)
print(f"   ‚Ä¢ Documentos de ejemplo: {len(sample_texts)}")
print(f"   ‚Ä¢ Chunks creados: {len(chunks)}")
print(f"   ‚Ä¢ Embeddings: {len(embeddings)}")
print(f"   ‚Ä¢ Dimensi√≥n vectorial: {embeddings[0].shape[0]}")
print(f"   ‚Ä¢ Vector store: FAISS")
print(f"   ‚Ä¢ Modelo embeddings: paraphrase-multilingual-MiniLM-L12-v2")
if llm:
    print(f"   ‚Ä¢ LLM: OpenAI GPT-3.5-turbo")
print("=" * 70)
print("\n‚ú® Demo completada exitosamente!")
